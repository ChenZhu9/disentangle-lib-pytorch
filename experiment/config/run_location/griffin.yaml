# @package _global_
logging:
  logs_dir: 'logs'

trainer:
  cuda: TRUE
  prepare_data_per_node: TRUE

dataset:
  num_workers: 32  # max 128, more than 16 doesn't really seem to help (tested on batch size 256*3)?
  data_root: '${oc.env:HOME}/workspace/research/disent/data/dataset'
  pin_memory: ${trainer.cuda}
  try_in_memory: TRUE
  gpu_augment: FALSE
  prepare: TRUE

hydra:
  job:
    name: 'disent'
  run:
    dir: '${logging.logs_dir}/hydra_run/${now:%Y-%m-%d_%H-%M-%S}_${hydra.job.name}'
  sweep:
    dir: '${logging.logs_dir}/hydra_sweep/${now:%Y-%m-%d_%H-%M-%S}_${hydra.job.name}'
    subdir: '${hydra.job.id}' # hydra.job.id is not available for dir
