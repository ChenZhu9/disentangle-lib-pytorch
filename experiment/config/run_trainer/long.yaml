# @package _global_
trainer:
  epochs: 100800
  steps: 100800
  cuda: TRUE
  prepare_data_per_node: TRUE

dataset:
  num_workers: 8
  batch_size: 256
  data_dir: '/tmp/${env:USER}/datasets'
  try_in_memory: FALSE
  gpu_augment: FALSE