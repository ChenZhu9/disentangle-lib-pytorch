# @package _group_
name: norm_conv64
weight_init: 'xavier_normal'
encoder:
  _target_: disent.model.ae.EncoderConv64Norm
  x_shape: ${dataset.x_shape}
  z_size: ${model.z_size}
  z_multiplier: ${framework.model_z_multiplier}
  activation: ${model.activation}
  norm: ${model.norm}
  norm_pre_act: ${model.norm_pre_act}
decoder:
  _target_: disent.model.ae.DecoderConv64Alt
  x_shape: ${dataset.x_shape}
  z_size: ${model.z_size}
  activation: ${model.activation}
  norm: ${model.norm}
  norm_pre_act: ${model.norm_pre_act}

# vars
activation: swish  # leaky_relu, relu
norm: layer  # batch, instance, layer, layer_chn, none
norm_pre_act: TRUE
