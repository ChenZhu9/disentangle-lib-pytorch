# @package _global_
framework:
  module:
    optimizer: torch_optimizer.AdaBelief
    optimizer_kwargs:
      lr: ${optimizer.lr}
      betas: [0.9, 0.999]
      eps: 1e-8
      weight_decay: 0

      amsgrad: False
      weight_decouple: False
      fixed_decay: False
      rectify: False
