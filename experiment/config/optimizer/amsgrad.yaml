# @package _global_
framework:
  module:
    optimizer: torch.optim.Adam
    optimizer_kwargs:
      lr: ${optimizer.lr}
      betas: [0.9, 0.999]
      eps: 1e-8
      weight_decay: 0

      amsgrad: True
